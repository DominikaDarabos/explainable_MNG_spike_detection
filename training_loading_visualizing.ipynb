{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from typing import Callable\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.ticker as mticker\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "\n",
    "from MicroneurographyDataloader import *\n",
    "from metrics import *\n",
    "from _external.WHVPNet_pytorch.networks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, \\\n",
    "          n_epoch: int, optimizer: torch.optim.Optimizer, \\\n",
    "          criterion: Callable[[torch.Tensor | list[torch.Tensor]], torch.Tensor], decision_boundary: float = 0.5) -> None:\n",
    "    n_digits = len(str(n_epoch))\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        total_length = 0\n",
    "\n",
    "        total_true_positives = 0\n",
    "        total_false_positives = 0\n",
    "        total_false_negatives = 0\n",
    "        total_true_negatives = 0\n",
    "\n",
    "        for data in data_loader:\n",
    "            input_data, binary_labels, multiple_labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_data)\n",
    "\n",
    "            binary_classes = binary_labels.argmax(dim=-1)\n",
    "            loss = criterion(outputs, binary_labels)\n",
    "           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predicted_labels = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "            predicted_classes = (predicted_labels[:, 1] > decision_boundary).float()\n",
    "\n",
    "            total_accuracy += (binary_classes == predicted_classes).sum().item()\n",
    "            total_length += binary_labels.size(0)\n",
    "            total_true_positives += ((binary_classes == 1) & (predicted_classes == 1)).sum().item()\n",
    "            total_false_positives += ((binary_classes == 0) & (predicted_classes == 1)).sum().item()\n",
    "            total_false_negatives += ((binary_classes == 1) & (predicted_classes == 0)).sum().item()\n",
    "            total_true_negatives += ((binary_classes == 0) & (predicted_classes == 0)).sum().item()\n",
    "\n",
    "\n",
    "        precision = total_true_positives / (total_true_positives + total_false_positives) if total_true_positives + total_false_positives > 0 else 0.0\n",
    "        recall = total_true_positives / (total_true_positives + total_false_negatives) if total_true_positives + total_false_negatives > 0 else 0.0\n",
    "        total_accuracy /= total_length / 100\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "        sensitivity = total_true_positives / (total_true_positives + total_false_negatives) if total_true_positives + total_false_negatives > 0 else 0.0\n",
    "        specificity = total_true_negatives / (total_true_negatives + total_false_positives) if total_true_negatives + total_false_positives > 0 else 0.0\n",
    "        balanced_accuracy = 0.5 * (sensitivity + specificity)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:0{n_digits}d} / {n_epoch}, '\n",
    "              f'accuracy: {total_accuracy:.2f}%, loss: {total_loss:.4f}, '\n",
    "              f'Precision: {precision:.4f}, Recall: {recall:.4f}, '\n",
    "              f'Balanced Accuracy: {balanced_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, \\\n",
    "         criterion: Callable[[torch.Tensor | list[torch.Tensor]], torch.Tensor], decision_boundary: float = 0.5) -> tuple[float, float]:\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        total_number = 0\n",
    "\n",
    "        all_binary_labels = []\n",
    "        all_multiple_labels = []\n",
    "        all_predicted_classes = []\n",
    "        all_predicted_probabilities = []\n",
    "\n",
    "        total_true_positives = 0\n",
    "        total_false_positives = 0\n",
    "        total_false_negatives = 0\n",
    "        total_true_negatives = 0\n",
    "        for data in data_loader:\n",
    "            input_data, binary_labels, multiple_labels = data\n",
    "            outputs = model(input_data)\n",
    "            loss = criterion(outputs, binary_labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            binary_classes = binary_labels.argmax(dim=-1)\n",
    "            predicted_labels = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "            predicted_classes = (predicted_labels[:, 1] > decision_boundary).float()\n",
    "\n",
    "            total_accuracy += (binary_classes == predicted_classes).sum().item()\n",
    "            total_number += binary_labels.size(0)\n",
    "            total_true_positives += ((binary_classes == 1) & (predicted_classes == 1)).sum().item()\n",
    "            total_false_positives += ((binary_classes == 0) & (predicted_classes == 1)).sum().item()\n",
    "            total_false_negatives += ((binary_classes == 1) & (predicted_classes == 0)).sum().item()\n",
    "            total_true_negatives += ((binary_classes == 0) & (predicted_classes == 0)).sum().item()\n",
    "\n",
    "\n",
    "            multiple_classes = multiple_labels.argmax(dim=-1)\n",
    "            all_multiple_labels.append(multiple_classes.cpu())\n",
    "            all_binary_labels.append(binary_classes.cpu())\n",
    "            all_predicted_classes.append(predicted_classes.cpu())\n",
    "            all_predicted_probabilities.append(predicted_labels.cpu())\n",
    "\n",
    "        all_binary_labels = torch.cat(all_binary_labels)\n",
    "        all_multiple_labels = torch.cat(all_multiple_labels)\n",
    "        all_predicted_classes = torch.cat(all_predicted_classes)\n",
    "        all_predicted_probabilities = torch.cat(all_predicted_probabilities)\n",
    "\n",
    "        sensitivity = total_true_positives / (total_true_positives + total_false_negatives) if total_true_positives + total_false_negatives > 0 else 0.0\n",
    "        specificity = total_true_negatives / (total_true_negatives + total_false_positives) if total_true_negatives + total_false_positives > 0 else 0.0\n",
    "        balanced_accuracy = 0.5 * (sensitivity + specificity)\n",
    "\n",
    "        compare_predictions_to_multilabels(all_binary_labels, all_multiple_labels, all_predicted_classes)\n",
    "\n",
    "        total_accuracy /= total_number / 100\n",
    "        print(\"=\" * 40)\n",
    "        print(f'Accuracy: {total_accuracy:.2f}%, loss: {total_loss:.4f}')\n",
    "        print(f'Weighted Balanced Accuracy: {balanced_accuracy:.4f}')\n",
    "        return total_accuracy, total_loss, all_binary_labels, all_multiple_labels, all_predicted_classes, all_predicted_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\darab\\Documents\\PROJECTS\\VP_microneurography\\code\\MicroneurographyDataloader.py:105: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  all_spike['track'] = all_spike['track'].replace(self.track_replacement_dict).infer_objects(copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Value statistics\n",
      "========================================\n",
      "Label 0: Overall Min = -10.230488, Overall Max = 9.450225\n",
      "Label 1: Overall Min = -10.242657, Overall Max = 9.476206\n",
      "Label 2: Overall Min = -9.928562, Overall Max = 9.464366\n",
      "Label 3: Overall Min = -8.4738655, Overall Max = 8.722748\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data generation and statistics with MNG dataloader\n",
    "\"\"\"\n",
    "MNG_dataloader = MicroneurographyDataloader()\n",
    "filename = 'window_15_overlap_11_corrected_or.pkl'\n",
    "MNG_dataloader.load_samples_and_labels_from_file(filename)\n",
    "# MNG_dataloader.generate_raw_windows(window_size=20, overlapping=15)\n",
    "# MNG_dataloader.generate_labels()\n",
    "# MNG_dataloader.generate_labels_stimuli_relabel()\n",
    "# MNG_dataloader.write_samples_and_labels_into_file(filename)\n",
    "\n",
    "# MNG_dataloader.get_statistics_of_spikes()\n",
    "# MNG_dataloader.plot_raw_data_window_by_label(0, 5)\n",
    "MNG_dataloader.get_value_statistics_for_classes()\n",
    "MNG_dataloader.plot_raw_data_window_by_label(1, 6)\n",
    "# MNG_dataloader.plot_raw_data_window_by_label(2, 5)\n",
    "# MNG_dataloader.plot_raw_data_window_by_label(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "First get the statistics of spikes to see what is the minimum gap between two timestamps. If the window size is not bigger than that, there will be no occurence of two spikes being in one window.\n",
    "\"\"\"\n",
    "MNG_dataloader = MicroneurographyDataloader(raw_data_relative_path='../data/5_nerve/raw_data.csv',\n",
    "                                            spikes_relative_path='../data/5_nerve/spike_timestamps.csv',\n",
    "                                            stimulation_relative_path='../data/5_nerve/stimulation_timestamps.csv')\n",
    "MNG_dataloader.get_statistics_of_spikes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\darab\\Documents\\PROJECTS\\VP_microneurography\\code\\MicroneurographyDataloader.py:105: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  all_spike['track'] = all_spike['track'].replace(self.track_replacement_dict).infer_objects(copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum index gap between spike timestamps: 13\n",
      "Maximum index gap between spike timestamps: 199982\n",
      "Minimum time gap between spike timestamps: 0.0010999999931300408\n",
      "Maximum time gap between spike timestamps: 4.000000000000455\n",
      "Minimum gap between sampled values: 9.999999929277692e-05\n",
      "Spike min gap / sample freq gap: 11.000000009094947\n",
      "Dataset loading.\n",
      "========================================\n",
      "Value statistics\n",
      "========================================\n",
      "Label 0: Overall Min = -9.671367, Overall Max = 8.843775\n",
      "Label 1: Overall Min = -9.671367, Overall Max = 8.832592\n",
      "Label 2: Overall Min = -6.2966104, Overall Max = 8.561269\n",
      "Label 3: Overall Min = -9.638477, Overall Max = 8.832592\n",
      "Label 4: Overall Min = -9.666762, Overall Max = 8.836868\n",
      "Label 5: Overall Min = -9.4043045, Overall Max = 8.84213\n",
      "Label 6: Overall Min = -8.598845, Overall Max = 8.814833\n",
      "========================================\n",
      "Epoch: 01 / 10, accuracy: 78.78%, loss: 25.8074, Precision: 0.5898, Recall: 0.4971, Balanced Accuracy: 0.6909\n",
      "Epoch: 02 / 10, accuracy: 89.23%, loss: 19.9364, Precision: 0.7676, Recall: 0.8167, Balanced Accuracy: 0.8671\n",
      "Epoch: 03 / 10, accuracy: 89.79%, loss: 19.4670, Precision: 0.7770, Recall: 0.8296, Balanced Accuracy: 0.8751\n",
      "Epoch: 04 / 10, accuracy: 90.00%, loss: 19.3543, Precision: 0.7815, Recall: 0.8329, Balanced Accuracy: 0.8776\n",
      "Epoch: 05 / 10, accuracy: 89.96%, loss: 19.3022, Precision: 0.7791, Recall: 0.8350, Balanced Accuracy: 0.8781\n",
      "Epoch: 06 / 10, accuracy: 90.00%, loss: 19.2545, Precision: 0.7795, Recall: 0.8368, Balanced Accuracy: 0.8789\n",
      "Epoch: 07 / 10, accuracy: 90.00%, loss: 19.2350, Precision: 0.7781, Recall: 0.8392, Balanced Accuracy: 0.8797\n",
      "Epoch: 08 / 10, accuracy: 90.11%, loss: 19.1989, Precision: 0.7808, Recall: 0.8404, Balanced Accuracy: 0.8809\n",
      "Epoch: 09 / 10, accuracy: 90.24%, loss: 19.1470, Precision: 0.7853, Recall: 0.8387, Balanced Accuracy: 0.8811\n",
      "Epoch: 10 / 10, accuracy: 90.23%, loss: 19.1517, Precision: 0.7835, Recall: 0.8417, Balanced Accuracy: 0.8821\n",
      "Parameter containing:\n",
      "tensor([ 0.3464, -0.3932, -0.0740,  1.8098, -2.4105, -1.8162], device='cuda:0',\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "========================================\n",
      "VALIDATION/TESTING:\n",
      "========================================\n",
      "Label 0:\n",
      "  TP: 0.0\n",
      "  FN: 0.0\n",
      "  FP: 14378.0\n",
      "  TN: 316885.0\n",
      "\n",
      "Label 1:\n",
      "  TP: 123.0\n",
      "  FN: 4.0\n",
      "  FP: 0.0\n",
      "  TN: 0.0\n",
      "\n",
      "Label 2:\n",
      "  TP: 68.0\n",
      "  FN: 59.0\n",
      "  FP: 0.0\n",
      "  TN: 0.0\n",
      "\n",
      "Label 3:\n",
      "  TP: 110.0\n",
      "  FN: 14.0\n",
      "  FP: 0.0\n",
      "  TN: 0.0\n",
      "\n",
      "Label 4:\n",
      "  TP: 116.0\n",
      "  FN: 6.0\n",
      "  FP: 0.0\n",
      "  TN: 0.0\n",
      "\n",
      "Label 5:\n",
      "  TP: 113.0\n",
      "  FN: 7.0\n",
      "  FP: 0.0\n",
      "  TN: 0.0\n",
      "\n",
      "Label 6:\n",
      "  TP: 76.0\n",
      "  FN: 46.0\n",
      "  FP: 0.0\n",
      "  TN: 0.0\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "Accuracy: 95.63%, loss: 9.3704\n",
      "Weighted Balanced Accuracy: 0.8867\n",
      "========================================\n",
      "           MODEL METRICS          \n",
      "========================================\n",
      "Precision     : 0.0404\n",
      "Recall        : 0.8167\n",
      "FPR           : 0.0434\n",
      "ROC-AUC       : 0.8867\n",
      "========================================\n",
      "       CONFUSION MATRIX           \n",
      "========================================\n",
      "              Predicted\n",
      "          316885    14378\n",
      "Actual    136    606\n",
      "========================================\n",
      "MERGED METRICS\n",
      "========================================\n",
      "recall 0.9171270718232044\n",
      "----------------------------------------\n",
      "GROUND TRUTH POSITIVE\n",
      "Quantity:  174\n",
      "Length : count\n",
      "4: 148\n",
      "5: 20\n",
      "8: 4\n",
      "9: 2\n",
      "----------------------------------------\n",
      "TRUE POSITIVE\n",
      "Quantity:  166\n",
      "Length : count\n",
      "1: 22\n",
      "2: 10\n",
      "3: 3\n",
      "4: 22\n",
      "5: 35\n",
      "6: 13\n",
      "7: 9\n",
      "8: 14\n",
      "9: 15\n",
      "10: 17\n",
      "11: 3\n",
      "12: 1\n",
      "14: 1\n",
      "15: 1\n",
      "----------------------------------------\n",
      "FALSE POSITIVE\n",
      "Quantity:  9026\n",
      "Length : count\n",
      "1: 6738\n",
      "2: 1038\n",
      "3: 324\n",
      "4: 550\n",
      "5: 280\n",
      "6: 55\n",
      "7: 18\n",
      "8: 14\n",
      "9: 6\n",
      "10: 3\n",
      "----------------------------------------\n",
      "FALSE NEGATIVE\n",
      "Quantity:  15\n",
      "Length : count\n",
      "4: 12\n",
      "5: 3\n",
      "----------------------------------------\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\darab\\Documents\\PROJECTS\\VP_microneurography\\code\\metrics.py:118: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(\n",
      "c:\\Users\\darab\\Documents\\PROJECTS\\VP_microneurography\\code\\metrics.py:118: UserWarning: \n",
      "The palette list has fewer values (3) than needed (6) and will cycle, which may produce an uninterpretable plot.\n",
      "  sns.boxplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCHANGE: model path for saving.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "FULL TRAINING\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "    CHANGE the decision boundary for the metrics here.\n",
    "\"\"\"\n",
    "decision_boundary = 0.8\n",
    "epoch = 10\n",
    "lr = 0.01\n",
    "dtype = torch.float64\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\"\"\"\n",
    "    CHANGE window size and overlapping size here.\n",
    "\"\"\"\n",
    "window_size = 13\n",
    "overlapping_size = 10\n",
    "\"\"\" \n",
    "    CHANGE the file name and path here for the already generated dataset.\n",
    "    If the file does not exist, the overlapping windows will be generater now and saved to this file path.\n",
    "\"\"\"\n",
    "MNG_dataloader_filepath = f'window_{window_size}_overlap_{overlapping_size}_corrected_5nerve.pkl'\n",
    "\n",
    "\"\"\"\n",
    "CHANGE the file source paths here.\n",
    "\"\"\"\n",
    "MNG_dataloader = MicroneurographyDataloader(raw_data_relative_path='../data/5_nerve/raw_data.csv',\n",
    "                                            spikes_relative_path='../data/5_nerve/spike_timestamps.csv',\n",
    "                                            stimulation_relative_path='../data/5_nerve/stimulation_timestamps.csv')\n",
    "\"\"\" \n",
    "CHANGE: this function prints the min distance between the spike instances. Useful for window size determination.\n",
    "\"\"\"\n",
    "MNG_dataloader.get_statistics_of_spikes()\n",
    "\n",
    "full_path = os.path.join('../data', MNG_dataloader_filepath)\n",
    "if os.path.exists(full_path):\n",
    "    print(\"Dataset loading.\")\n",
    "    MNG_dataloader.load_samples_and_labels_from_file(MNG_dataloader_filepath)\n",
    "else:\n",
    "    print(\"Dataset generating.\")\n",
    "    MNG_dataloader.generate_raw_windows(window_size=window_size, overlapping=overlapping_size)\n",
    "    \"\"\" \n",
    "    CHANGE If no relabel is necessary.\n",
    "    \"\"\"\n",
    "    #dataSet.generate_labels()\n",
    "    # on the first dataset with two nerves, negative_stimulus_limit=-10, positive_stimulus_limit=9 works great\n",
    "    \"\"\" \n",
    "    CHANGE the filtering thresholds here.\n",
    "    Unfortunately, it needs a little testing.\n",
    "    the filter gets windows which has values {negative_stimulus_limit} {logigal_operator} {positive_stimulus_limit}\n",
    "    For the first file, -10 and 9 was fine.\n",
    "    For this file, -9 or 8 is fine.\n",
    "    If after relabel, the number of stimulus relabels are approximately the same as the number of stimulus windows there was before, the relabel is correct.\n",
    "    \"\"\"\n",
    "    MNG_dataloader.generate_labels_stimuli_relabel(logigal_operator=\"or\")\n",
    "    MNG_dataloader.write_samples_and_labels_into_file(MNG_dataloader_filepath)\n",
    "\n",
    "\"\"\" \n",
    "    CHANGE the over and undersampling ratio here. \n",
    "\"\"\"\n",
    "dataloaders = MNG_dataloader.sequential_split_with_resampling(batch_size=1024, minor_upsample_count=25000, major_downsample_count=75000)\n",
    "\n",
    "\"\"\" \n",
    "CHANGE: this function prints the min and max value for every multi class label. Also useful for stimuli relabel.\n",
    "\"\"\"\n",
    "MNG_dataloader.get_value_statistics_for_classes()\n",
    "\"\"\" \n",
    "CHANGE: this function prints how many labels are  for every multi class label.\n",
    "If the number of stimulus labels are not in correlation with the number of other labels, the stimulus filter should be changed by the logical operator.\n",
    "\"\"\"\n",
    "MNG_dataloader.get_statistics_of_labels()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# WHVPNet params\n",
    "samples = torch.tensor(MNG_dataloader.raw_data_windows, dtype=torch.float64).unsqueeze(1)#.to(MNG_dataloader.device)\n",
    "n_channels, n_in = samples[0].shape\n",
    "n_out = len(MNG_dataloader.binary_labels_onehot[0])\n",
    "num_VP_features = 6\n",
    "num_weights = 4\n",
    "fcn_neurons = 6\n",
    "affin = torch.tensor([6 / n_in, -0.3606]).tolist()\n",
    "weight = ((torch.rand(num_weights)-0.5)*8).tolist()\n",
    "\n",
    "\n",
    "model = VPNet(n_in, n_channels, num_VP_features, VPTypes.FEATURES, affin + weight, WeightedHermiteSystem(n_in, num_VP_features, num_weights), [fcn_neurons], n_out, device=device, dtype=dtype)\n",
    "\"\"\" \n",
    "CHANGE the weigths here for the class imbalances. They should sum up to 1, and should be in close correlation with the oversampling undersampling ratio.\n",
    "\"\"\"\n",
    "class_weights = torch.tensor([0.3, 0.7]).to(device)\n",
    "weighted_criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "criterion = VPLoss(weighted_criterion, 0.1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, dataloaders['train_loader_under'], epoch, optimizer, criterion)\n",
    "\n",
    "if isinstance(model, VPNet):\n",
    "    print(*list(model.vp_layer.parameters()))\n",
    "\n",
    "class_weights = torch.tensor([0.003, 0.997]).to(device)\n",
    "weighted_criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "criterion = VPLoss(weighted_criterion, 0.1)\n",
    "print(\"=\" * 40)\n",
    "print(\"VALIDATION/TESTING:\")\n",
    "\"\"\"\n",
    "CHANGE the dataloader on which the validation should be.\n",
    "dataloaders['val_loader'] is for validation dataset.\n",
    "dataloaders['test_loader'] If you finished with the training, use this test set for the function.\n",
    "\"\"\"\n",
    "accuracy, loss, all_binary_labels, all_multiple_labels, all_predicted_classes, all_predicted_probabilities = test(model, dataloaders['val_loader'], criterion, decision_boundary)\n",
    "compute_common_metrics(all_binary_labels, all_predicted_classes)\n",
    "compute_merged_metrics(all_binary_labels, all_predicted_classes)\n",
    "create_decision_ceratinty_boxplots(all_binary_labels, all_multiple_labels, all_predicted_classes, all_predicted_probabilities)\n",
    "print()\n",
    "\n",
    "\"\"\"\n",
    "CHANGE: model path for saving.\n",
    "\"\"\"\n",
    "#torch.save(model.state_dict(), f'_trained_models/widnow_{window_size_}_overlapping_{overlapping_size_}_hidden_{hidden1}_nweight_{weight_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\darab\\Documents\\PROJECTS\\VP_microneurography\\code\\MicroneurographyDataloader.py:105: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  all_spike['track'] = all_spike['track'].replace(self.track_replacement_dict).infer_objects(copy=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load pretrained model.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "CHANGE the path of the trained model here.\n",
    "\"\"\"\n",
    "model_name = '_trained_models/widnow_15_overlapping_11_hidden_6_nweight_4_id_6'\n",
    "\"\"\"\n",
    "    CHANGE the parameters to which the pretrained model was trained.\n",
    "\"\"\"\n",
    "dtype = torch.float64\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "window_size = 15\n",
    "overlapping_size = 11\n",
    "\n",
    "MNG_dataloader = MicroneurographyDataloader()\n",
    "path = f'window_{window_size}_overlap_{overlapping_size}_corrected.pkl'\n",
    "MNG_dataloader.load_samples_and_labels_from_file(path)\n",
    "\n",
    "dataloaders = MNG_dataloader.sequential_split_with_resampling(1024)\n",
    "\n",
    "sample_windows = torch.tensor(MNG_dataloader.raw_data_windows, dtype=torch.float64).unsqueeze(1)\n",
    "n_channels, n_in = sample_windows[0].shape\n",
    "n_out = len(MNG_dataloader.binary_labels_onehot[0])\n",
    "num_VP_features = 6\n",
    "num_weights = 4\n",
    "fcn_neurons = 6\n",
    "affin = torch.tensor([6 / n_in, -0.3606]).tolist()\n",
    "weight = ((torch.rand(num_weights)-0.5)*8).tolist()\n",
    "\n",
    "\n",
    "model = VPNet(n_in, n_channels, num_VP_features, VPTypes.FEATURES, affin + weight, WeightedHermiteSystem(n_in, num_VP_features, num_weights), [fcn_neurons], n_out, device=device, dtype=dtype)\n",
    "model.load_state_dict(torch.load(model_name, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Label 0:\n",
      "  TP: 0.0\n",
      "  FN: 0.0\n",
      "  FP: 15258.0\n",
      "  TN: 387548.0\n",
      "\n",
      "Label 1:\n",
      "  TP: 259.0\n",
      "  FN: 0.0\n",
      "  FP: 0.0\n",
      "  TN: 0.0\n",
      "\n",
      "Label 2:\n",
      "  TP: 255.0\n",
      "  FN: 0.0\n",
      "  FP: 0.0\n",
      "  TN: 0.0\n",
      "\n",
      "Label 3:\n",
      "  TP: 236.0\n",
      "  FN: 17.0\n",
      "  FP: 0.0\n",
      "  TN: 0.0\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "Accuracy: 96.22%, loss: 10.2869\n",
      "Weighted Balanced Accuracy: 0.9700\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation.\n",
    "\"\"\"\n",
    "\"\"\" \n",
    "CHANGE: the dataset name to val for validation and test for testing. Adjust the decision boundary.\n",
    "It can also be explanatory, the model does not need retraining only for a decision boundary adjustment.\n",
    "\"\"\"\n",
    "dataset_name='val' # 'or test\n",
    "decision_boundary = 0.8\n",
    "class_weights = torch.tensor([0.003, 0.997]).to(device)\n",
    "weighted_criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "criterion = VPLoss(weighted_criterion, 0.1)\n",
    "accuracy, loss, binary_labels, multiple_labels, predicted_classes, predicted_probabilities = test(model, dataloaders[f'{dataset_name}_loader'], criterion, decision_boundary)\n",
    "# compute_common_metrics(binary_labels, predicted_classes)\n",
    "# compute_merged_metrics(binary_labels, predicted_classes)\n",
    "# create_decision_ceratinty_boxplots(binary_labels, multiple_labels, predicted_classes, predicted_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get samples and timestamps from the dataloader.\n",
    "\"\"\"\n",
    "\n",
    "sample_windows = []\n",
    "for data in dataloaders[f'{dataset_name}_loader']:\n",
    "    x, labels, multiple = data\n",
    "    sample_windows.append(x.cpu())\n",
    "sample_windows = torch.cat(sample_windows).squeeze(1)\n",
    "timestamp_windows = dataloaders[f'{dataset_name}_timestamps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieve information for the output plot.\n",
    "\"\"\"\n",
    "\n",
    "def reconstruct_original_sequence(overlapping_windows, window_size, overlapping):\n",
    "    \"\"\"\n",
    "    Transforms back the overlapping windows into a one dimensional sequence.\n",
    "    \"\"\"\n",
    "    overlapping_windows = np.asarray(overlapping_windows)\n",
    "    num_windows = len(overlapping_windows)\n",
    "    stride = window_size - overlapping\n",
    "    original_length = (num_windows - 1) * stride + window_size\n",
    "\n",
    "    reconstructed_sequence = np.zeros(original_length)\n",
    "    for i in range(num_windows):\n",
    "        start_index = i * stride\n",
    "        end_index = start_index + window_size\n",
    "        reconstructed_sequence[start_index:end_index] = overlapping_windows[i]\n",
    "    reconstructed_sequence = np.nan_to_num(reconstructed_sequence)\n",
    "    return reconstructed_sequence\n",
    "\n",
    "\n",
    "original_samples_seq = reconstruct_original_sequence(sample_windows, window_size, overlapping_size)\n",
    "original_timestamps_seq = reconstruct_original_sequence(timestamp_windows, window_size, overlapping_size)\n",
    "\n",
    "start_ts = original_timestamps_seq[0]\n",
    "end_ts = original_timestamps_seq[-1]\n",
    "start_index = MNG_dataloader.all_spikes_df['ts'].searchsorted(start_ts, side='left')\n",
    "end_index = MNG_dataloader.all_spikes_df['ts'].searchsorted(end_ts, side='right')\n",
    "ground_truth_spikes_in_dataset = MNG_dataloader.all_spikes_df.iloc[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_predictions_to_sequence(overlapping_windows, window_size, overlapping):\n",
    "    \"\"\"\n",
    "    Transforms the overlapping windows with prediction probabilities back into a one dimensional sequence.\n",
    "    Every datapoint got different probabilities from the different windows it was present in.\n",
    "    The sequence contains for every datapoint the average probs of all the probs it got in the repetitive windows.\n",
    "    \"\"\"\n",
    "    predicted_probs_np = np.asarray(predicted_probabilities[:, 1])\n",
    "    overlapping_windows = np.asarray(overlapping_windows)\n",
    "    num_windows = len(overlapping_windows)\n",
    "    stride = window_size - overlapping\n",
    "    original_length = (num_windows - 1) * stride + window_size\n",
    "\n",
    "    transformed_sequence = np.zeros(original_length)\n",
    "    count_array = np.zeros(original_length)\n",
    "    for i in range(num_windows):\n",
    "        start_index = i * stride\n",
    "        end_index = start_index + window_size\n",
    "        transformed_sequence[start_index:end_index] += predicted_probs_np[i]\n",
    "        count_array[start_index:end_index] += 1\n",
    "    non_zero_count_mask = count_array > 0\n",
    "    transformed_sequence[non_zero_count_mask] /= count_array[non_zero_count_mask] #avg of the predictions one datapoint got from all windows it was present in\n",
    "    transformed_sequence = np.nan_to_num(transformed_sequence)\n",
    "    return transformed_sequence\n",
    "\n",
    "prediction_sequence = transform_predictions_to_sequence(timestamp_windows, 15, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ground truth spikes present between timestamps 2068.1215416 - 2068.4214416\n",
      "No ground truth spikes present between timestamps 2068.4215416 - 2068.7214416\n"
     ]
    }
   ],
   "source": [
    "def plot_output_windows():\n",
    "    \"\"\"\n",
    "    sample_size : how many datapoints should be included in one plot.\n",
    "    plotting_start_idx = from where the plotting should start.\n",
    "    num_of_plots = the number of plots the code should generate after each other.\n",
    "    \"\"\"\n",
    "    sample_size = 3000\n",
    "    plotting_start_idx = 181200\n",
    "    num_of_plots = 2\n",
    "\n",
    "    # probability bins for the coloring and legend\n",
    "    prob_bin_colors = ['#C0C0C0', 'khaki', '#FFA500', '#FF0000']\n",
    "    bin_limits = [0, 50, 80, 98, 100]\n",
    "    percentage_labels = ['0-50%', '50-80%', '80-98%', '98-100%']\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=label, markerfacecolor=prob_bin_colors[i], markersize=10)\n",
    "                       for i, label in enumerate(percentage_labels)]\n",
    "\n",
    "    # spikes for legend\n",
    "    unique_tracks = MNG_dataloader.track_replacement_dict.values()\n",
    "    marker_styles = ['s', '^', 'D', 'p', '*', 'h', 'H', '+', 'x', '|', '_', 'v', '<', '>', '8', 'P', 'X']\n",
    "    track_markers = {track: marker_styles[i % len(marker_styles)] for i, track in enumerate(unique_tracks)}\n",
    "    spike_labels = [f'AP {track-1}' if track != 1 else 'Stimulus' for track in unique_tracks]\n",
    "\n",
    "    track_legend_elements = [\n",
    "        Line2D([0], [0], color='black', marker=track_markers[track_num], linestyle='None', markersize=10, label=label)\n",
    "        for track_num, label in zip(unique_tracks, spike_labels)\n",
    "    ]\n",
    "\n",
    "    for start_index in range(plotting_start_idx, plotting_start_idx+sample_size*num_of_plots, sample_size):\n",
    "        end_index = start_index + sample_size\n",
    "        timesamps_to_plot_np = np.asarray(original_timestamps_seq[start_index:end_index])\n",
    "        samples_to_plot_np = original_samples_seq[start_index:end_index]\n",
    "        probs_to_plot_np = prediction_sequence[start_index:end_index]\n",
    "\n",
    "        plt.figure(figsize=(20, 6))\n",
    "\n",
    "        # Define color bins based on val_probabilities_class1\n",
    "        probability_map = np.digitize(probs_to_plot_np * 100, bins=bin_limits) - 1\n",
    "        probability_map = np.clip(probability_map, 0, len(prob_bin_colors) - 1)\n",
    "\n",
    "        ts_sample_points = np.array([timesamps_to_plot_np, samples_to_plot_np]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([ts_sample_points[:-1], ts_sample_points[1:]], axis=1)\n",
    "        line_colors = [prob_bin_colors[probability_map[i]] for i in range(len(probability_map) - 1)]\n",
    "        lc = LineCollection(segments, colors=line_colors, linewidths=2, alpha=0.6)\n",
    "        ax = plt.gca()\n",
    "        ax.add_collection(lc)\n",
    "\n",
    "        ground_truth_spikes_to_plot = ground_truth_spikes_in_dataset[(ground_truth_spikes_in_dataset['ts'] >= timesamps_to_plot_np.min()) & \n",
    "                                              (ground_truth_spikes_in_dataset['ts'] <= timesamps_to_plot_np.max())]\n",
    "\n",
    "        if ground_truth_spikes_to_plot.empty:\n",
    "            print(f\"No ground truth spikes present between timestamps {timesamps_to_plot_np.min()} - {timesamps_to_plot_np.max()}\")\n",
    "            plt.plot()\n",
    "\n",
    "        y_top = samples_to_plot_np.max() * 1.1\n",
    "        y_bottom = samples_to_plot_np.min() * 1.1\n",
    "\n",
    "        track_marker_map = {track: marker_styles[i % len(marker_styles)] for i, track in enumerate(unique_tracks)}\n",
    "        for index, spike_row in ground_truth_spikes_to_plot.iterrows():\n",
    "            color = 'black'\n",
    "            marker = track_marker_map.get(int(spike_row['track']), 'o')\n",
    "\n",
    "            # vertical marks for the spikes\n",
    "            plt.scatter(spike_row['ts'], y_top, color=color, marker=marker, s=120)\n",
    "            plt.scatter(spike_row['ts'], y_bottom, color=color, marker=marker, s=120, label=spike_row['track'])\n",
    "            plt.axvline(x=spike_row['ts'], color=color, linestyle='--', lw=0.5)\n",
    "        \n",
    "        y_min_all = original_samples_seq.min() * 1.2\n",
    "        y_max_all = original_samples_seq.max() * 1.2\n",
    "        plt.ylim(y_min_all, y_max_all)\n",
    "        plt.xlim(timesamps_to_plot_np.min(), timesamps_to_plot_np.max())\n",
    "        plt.legend(handles=legend_elements + track_legend_elements, title=\"Probability and AP\", bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=14, title_fontsize=16)\n",
    "        plt.xlabel('Timestamp', fontsize=14)\n",
    "        plt.ylabel('Amplitude', fontsize=14)\n",
    "        plt.grid(axis='y', linestyle=':', linewidth=1)\n",
    "        plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(nbins=50))\n",
    "        plt.gca().xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x:.4f}'))\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "plot_output_windows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microneurography",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
